{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Numpy Study Notes"
      ],
      "metadata": {
        "id": "-SvgPpvXKcDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numpy Study Notes with an emphasis on Large Language Models and Computer Vision applications"
      ],
      "metadata": {
        "id": "aKfO7IdNUQd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "B24fGGYaUbEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 1 . Numpy section one test https://gemini.google.com/share/99dbfcff7f9f"
      ],
      "metadata": {
        "id": "a5Inru8c0on7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Gsf-kP5fUcj7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. np.array(data): Creates an array from a list or tuple.\n",
        "# Converting a Python list into a high-performance vector.\n",
        "vector = np.array([1, 2, 3])\n",
        "print(f\"1. Array:\\n{vector}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4ZgMNazlbwV",
        "outputId": "94893e61-484e-4ea3-8975-f5047bf4802a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Array:\n",
            "[1 2 3]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MyArray = np.array([[1,2,3], [2,3,4]])\n",
        "print(MyArray[0][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYmoqIEOrj4L",
        "outputId": "448b5550-00cc-403c-8082-77bdb200fe24"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. np.zeros(shape): Initializes an array of zeros.\n",
        "# Useful for padding or mask initialization.\n",
        "zeros_mask = np.zeros((2, 3))\n",
        "print(f\"2. Zeros (2x3):\\n{zeros_mask}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LhD5aDAlhBG",
        "outputId": "763c8a7d-1616-45d4-f26e-22ba239de43c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. Zeros (2x3):\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. np.ones(shape): Initializes an array of ones.\n",
        "ones_tensor = np.ones((1, 5))\n",
        "print(f\"3. Ones (1x5):\\n{ones_tensor}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuBDWCxdllsy",
        "outputId": "d7db34d8-856c-4dc3-a5b1-20a8b4426891"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. Ones (1x5):\n",
            "[[1. 1. 1. 1. 1.]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. np.full(shape, fill_value): Creates a constant array.\n",
        "# Often used to fill masks with -inf before Softmax.\n",
        "neg_inf_mask = np.full((2, 2), -np.inf)\n",
        "print(f\"4. Full (-inf):\\n{neg_inf_mask}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3jCMtz2lqB4",
        "outputId": "971330cb-0f55-49ac-959a-b4bb9d6e0b5a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. Full (-inf):\n",
            "[[-inf -inf]\n",
            " [-inf -inf]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. np.eye(N): Creates an identity matrix.\n",
        "# Often used for initializing orthogonal weights.\n",
        "identity = np.eye(3)\n",
        "print(f\"5. Identity (3x3):\\n{identity}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIu2JXgEl-Vt",
        "outputId": "0ffb941a-4972-49ea-df0b-5230700609d1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5. Identity (3x3):\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. np.arange(start, stop, step): Returns evenly spaced values.\n",
        "# Great for positional encoding indices.\n",
        "indices = np.arange(0, 10, 2)\n",
        "print(f\"6. Arange (0 to 10 step 2):\\n{indices}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wqxEkz0nVlK",
        "outputId": "c299e3da-0ce0-4af4-d612-82e8cfd47438"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6. Arange (0 to 10 step 2):\n",
            "[0 2 4 6 8]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. np.linspace(start, stop, num): num samples over an interval.\n",
        "linear_space = np.linspace(0, 1, 5)\n",
        "print(f\"7. Linspace (5 points between 0-1):\\n{linear_space}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSEsYKtFnWYo",
        "outputId": "ad09cee1-8e57-41bd-ad64-e654fa91ec6f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7. Linspace (5 points between 0-1):\n",
            "[0.   0.25 0.5  0.75 1.  ]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. np.empty(shape): Allocates memory without initializing.\n",
        "# Fastest for large pre-allocations (contains 'garbage' values).\n",
        "empty_alloc = np.empty((2, 2))\n",
        "print(f\"8. Empty (Uninitialized):\\n{empty_alloc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS6XgREHnbEq",
        "outputId": "0464b30a-373e-44fd-9f98-9f92affa0677"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8. Empty (Uninitialized):\n",
            "[[0.25 0.5 ]\n",
            " [0.75 1.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is exactly why it’s dangerous and how it can subtly sabotage your models.\n",
        "1. It contains \"Memory Garbage\"When you call np.zeros() or np.ones(), NumPy talks to your Operating System (OS), reserves a block of RAM, and then spends time writing $0$s or $1$s over every single bit in that block.When you call np.empty(), NumPy reserves the RAM but skips the writing step. It just hands you the keys to that memory block exactly as it was left by the previous occupant (another program, a closed browser tab, or a previous variable).\n",
        "\n",
        "2. The \"Silent Failure\" BugThe biggest danger is that your code might actually run without throwing an error, but produce nonsensical results.Scenario: You create an array for a mask using np.empty().The Bug: You forget to fill the last row of the mask.The Result: Instead of that row being $0$, it contains a \"garbage\" value like $6.23 \\times 10^{-307}$. In a Neural Network, this tiny value might be treated as a real signal, leading to exploding gradients or \"NaN\" losses that are nearly impossible to trace back to a single uninitialized row.\n",
        "\n",
        "3. Non-Deterministic BehaviorBecause np.empty() pulls whatever is currently in your RAM, the \"garbage\" values change every time you run the script.Run 1: Your code works because the garbage values happened to be very small.Run 2: Your code crashes because the garbage values happened to be massive.This makes debugging a nightmare because you cannot consistently reproduce the failure.\n",
        "\n",
        "When should you actually use it?Despite the risks, np.empty() exists for a reason: Performance.If you are allocating a massive 10GB tensor that you are immediately going to overwrite (e.g., by loading data from a disk or a camera feed), np.empty() saves you the time it takes to write $0$s to 10GB of memory—which can take several seconds.Rule of Thumb: Use np.empty() only if the very next line of code is a function that completely fills that array (like np.copyto(), np.random.randn(), or a full-slice assignment)."
      ],
      "metadata": {
        "id": "ClRycbcSt13l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.zeros((8, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "ZPXsBY-Nxfes",
        "outputId": "c9058e65-2262-4a59-e0b2-625780aa5f0d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot interpret '32' as a data type",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1556402702.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '32' as a data type"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.eye(5, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIHI50rozKeC",
        "outputId": "eaa0b69e-ecae-40b1-b74d-dd9f9ddc7744"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.eye(5, k = 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAcyl_E2zRLV",
        "outputId": "78d4eb5a-2838-426b-9d90-624c8c45c49d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2 - Shape Manipulation and Transposition"
      ],
      "metadata": {
        "id": "oeg9i74snhGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "get the gemini questions here https://gemini.google.com/share/1cc643ca3f88"
      ],
      "metadata": {
        "id": "AgT25sWmf4kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data: A simulated \"batch\" of 2 sequences, each with 3 tokens, and 4 embedding dimensions\n",
        "# Shape: (2, 3, 4) -> (Batch, Seq_Len, Embedding_Dim)\n",
        "data = np.arange(24).reshape(2, 3, 4)\n",
        "print(data)\n",
        "print(data.ndim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQYteucI0Fb9",
        "outputId": "c883d2f6-6aac-480e-87de-fa6c5c0cfe14"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0  1  2  3]\n",
            "  [ 4  5  6  7]\n",
            "  [ 8  9 10 11]]\n",
            "\n",
            " [[12 13 14 15]\n",
            "  [16 17 18 19]\n",
            "  [20 21 22 23]]]\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped = np.reshape(data, (2, 12))\n",
        "print(reshaped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haniKmgnOBvv",
        "outputId": "d85ecbac-59a1-4233-d19f-ba2d56abf8d7"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17 18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transposed = np.transpose(data, axes = (0, 2, 1))\n",
        "print(transposed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQCYVmbcOSun",
        "outputId": "da4346bb-3a82-4a91-85b9-f2029203e2fb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0  4  8]\n",
            "  [ 1  5  9]\n",
            "  [ 2  6 10]\n",
            "  [ 3  7 11]]\n",
            "\n",
            " [[12 16 20]\n",
            "  [13 17 21]\n",
            "  [14 18 22]\n",
            "  [15 19 23]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = np.array([[1, 2], [3, 4]])\n",
        "print(matrix.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccBf6iF1PKxt",
        "outputId": "103bd285-5105-48c7-a015-d745f60455c1"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 3]\n",
            " [2 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = np.array([1, 2, 3, 4])\n",
        "expanded1 = np.expand_dims(vector, axis=0)\n",
        "print(expanded1)\n",
        "expanded2 = np.expand_dims(vector, axis=1)\n",
        "print(expanded2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG60gOcpPbCj",
        "outputId": "fa49a1e6-be89-4b81-dafe-5b3fb29fccde"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3 4]]\n",
            "[[1]\n",
            " [2]\n",
            " [3]\n",
            " [4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expanded1.reshape(2, -1)\n",
        "print(expanded1)\n",
        "print(expanded1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV6mBu5mYXVE",
        "outputId": "d4cc71c8-8bd0-40f8-c262-a42bece9a590"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3 4]]\n",
            "(1, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "squeezed1 = np.squeeze(expanded1, axis=0)\n",
        "squeezed2 = np.squeeze(expanded2, axis=1)\n",
        "print(squeezed1)\n",
        "print(squeezed2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgeWNoInP5Ap",
        "outputId": "8db7cce6-84fc-46b2-ecc5-a6200daf4f26"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3 4]\n",
            "[1 2 3 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a[:, None ] Method to add dimensions like newaxis."
      ],
      "metadata": {
        "id": "4wmuzgLSXq3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.ascontiguousarray() - called after transpose because CUDA may require contiguis memory placement."
      ],
      "metadata": {
        "id": "Rh9cK4fYchDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(squeezed1)\n",
        "print(squeezed1[:, None])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhq1E8YPXRZy",
        "outputId": "f4a74c02-8c44-40ec-8383-de5ce0c95814"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3 4]\n",
            "[[1]\n",
            " [2]\n",
            " [3]\n",
            " [4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a, b = np.ones((2, 2)), np.zeros((2, 2))\n",
        "concat = np.concatenate((a, b), axis=1)\n",
        "print(concat)\n",
        "concat2 = np.concatenate((a, b), axis=0)\n",
        "print(concat2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QYLVtjsQSdE",
        "outputId": "6b3b0aa0-95d6-4412-9b6c-2b9727c18265"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1. 0. 0.]\n",
            " [1. 1. 0. 0.]]\n",
            "[[1. 1.]\n",
            " [1. 1.]\n",
            " [0. 0.]\n",
            " [0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New Axis"
      ],
      "metadata": {
        "id": "VC15KZLeW97L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(concat2.shape)\n",
        "concat2[np.newaxis, np.newaxis, :, :].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pz2HqcrWs4k",
        "outputId": "779c310f-addc-4f83-82bf-dcd6a3aca10b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 4, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stacked = np.stack([a, b], axis=0)\n",
        "print(stacked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLJAVWq5Q-1y",
        "outputId": "09c78155-e463-4296-844d-f1ef51c861e5"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1. 1.]\n",
            "  [1. 1.]]\n",
            "\n",
            " [[0. 0.]\n",
            "  [0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.stack vs np.concatenate\n",
        "This is a very common interview question:\n",
        "\n",
        "np.concatenate: Glues arrays together along an existing axis. (Total dimensions stay the same).\n",
        "\n",
        "np.stack: Glues arrays together along a new axis. (Total dimensions increase by 1)."
      ],
      "metadata": {
        "id": "ENGpqoSORmWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Computer Vision, individual images are usually stored as 3D arrays: $(Height, Width, Channels)$. To train a model, you must \"stack\" these individual images into a 4D Batch: $(Batch, Height, Width, Channels)$.Here is the code to simulate loading 3 individual images and preparing them for a model:"
      ],
      "metadata": {
        "id": "alGfKGdnR6TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulate 3 individual RGB images (32x32 pixels)\n",
        "img1 = np.random.randint(0, 255, (32, 32, 3))\n",
        "img2 = np.random.randint(0, 255, (32, 32, 3))\n",
        "img3 = np.random.randint(0, 255, (32, 32, 3))\n",
        "\n",
        "print(img1)\n",
        "\n",
        "print(f\"Single image shape: {img1.shape}\")\n",
        "\n",
        "# Stack them into a batch at axis 0\n",
        "batch = np.stack([img1, img2, img3], axis=0)\n",
        "\n",
        "print(f\"Batch shape: {batch.shape}\")\n",
        "# Output: (3, 32, 32, 3) -> (Batch Size, Height, Width, Channels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJV3PthHR4d4",
        "outputId": "1a2a0920-e967-4269-a10a-850dd859e4dc"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[149 101   6]\n",
            "  [ 14 223 138]\n",
            "  [ 23 254 194]\n",
            "  ...\n",
            "  [ 40  93  56]\n",
            "  [249 163  63]\n",
            "  [240  97  15]]\n",
            "\n",
            " [[ 87  77  89]\n",
            "  [ 71  13  30]\n",
            "  [ 65 111  67]\n",
            "  ...\n",
            "  [  4 230 200]\n",
            "  [116 145 148]\n",
            "  [ 71 235 243]]\n",
            "\n",
            " [[225  73  50]\n",
            "  [168 248  23]\n",
            "  [231 151 247]\n",
            "  ...\n",
            "  [170  35 166]\n",
            "  [188 143 121]\n",
            "  [229  58  95]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 18 147 214]\n",
            "  [234  26  93]\n",
            "  [ 84  43 162]\n",
            "  ...\n",
            "  [154 187 210]\n",
            "  [ 48  64 165]\n",
            "  [ 82 179   0]]\n",
            "\n",
            " [[167  80  20]\n",
            "  [ 85  14  49]\n",
            "  [230  47 230]\n",
            "  ...\n",
            "  [ 29 107  72]\n",
            "  [108 240 144]\n",
            "  [ 99  13  18]]\n",
            "\n",
            " [[ 70  58 194]\n",
            "  [ 96  43 159]\n",
            "  [110  10 105]\n",
            "  ...\n",
            "  [196  64  45]\n",
            "  [ 17 140 152]\n",
            "  [231   7  73]]]\n",
            "Single image shape: (32, 32, 3)\n",
            "Batch shape: (3, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why axis=0 is critical here:\n",
        "By choosing axis=0, you ensure that the first number in the shape tuple is the index of the image. When you want to access the second image, you just call batch[1].\n",
        "\n",
        "If you had used axis=-1 (the last axis), your shape would be (32, 32, 3, 3), which would mean every pixel now has 9 color values—a complete spatial mess!"
      ],
      "metadata": {
        "id": "H7A2MyYlSMLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_val = np.array([[1]])\n",
        "repeated = np.repeat(single_val, 3, axis=1)\n",
        "print(repeated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUtrj9d4TwcF",
        "outputId": "4cce67b8-40cc-4655-ccd4-630998abf5be"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data: A simulated \"batch\" of 2 sequences, each with 3 tokens, and 4 embedding dimensions\n",
        "# Shape: (2, 3, 4) -> (Batch, Seq_Len, Embedding_Dim)\n",
        "data = np.arange(24).reshape(2, 3, 4)\n",
        "\n",
        "print(\"--- 2. Shape Manipulation and Transposition ---\")\n",
        "\n",
        "# 1. np.reshape(a, newshape): Changes shape without changing data.\n",
        "# Flattening the tokens for a linear layer: (2, 3, 4) -> (2, 12)\n",
        "reshaped = np.reshape(data, (2, 12))\n",
        "print(f\"1. Reshaped (2x12):\\n{reshaped.shape}\\n\")\n",
        "\n",
        "# 2. np.transpose(a, axes): Permutes dimensions.\n",
        "# Swapping Seq_Len and Embedding_Dim for Attention (Batch, Dim, Seq): (2, 4, 3)\n",
        "transposed = np.transpose(data, (0, 2, 1))\n",
        "print(f\"2. Transposed (0, 2, 1):\\n{transposed.shape}\\n\")\n",
        "\n",
        "# 3. a.T: Shortcut for 2D transposition.\n",
        "matrix = np.array([[1, 2], [3, 4]])\n",
        "print(f\"3. Matrix Transpose:\\n{matrix.T}\\n\")\n",
        "\n",
        "# 4. np.expand_dims(a, axis): Adds a dimension.\n",
        "# Turning a 1D vector into a \"batch of 1\": (4,) -> (1, 4)\n",
        "vector = np.array([1, 2, 3, 4])\n",
        "expanded = np.expand_dims(vector, axis=0)\n",
        "print(f\"4. Expanded Dims:\\n{expanded.shape}\\n\")\n",
        "\n",
        "# 5. np.squeeze(a, axis): Removes single-dimensional entries.\n",
        "# Removing the batch dimension: (1, 4) -> (4,)\n",
        "squeezed = np.squeeze(expanded, axis=0)\n",
        "print(f\"5. Squeezed:\\n{squeezed.shape}\\n\")\n",
        "\n",
        "# 6. np.concatenate((a, b), axis): Joins arrays along an existing axis.\n",
        "# Merging two hidden states side-by-side.\n",
        "a, b = np.ones((2, 2)), np.zeros((2, 2))\n",
        "concat = np.concatenate((a, b), axis=1)\n",
        "print(f\"6. Concatenated (axis 1):\\n{concat}\\n\")\n",
        "\n",
        "# 7. np.stack(arrays, axis): Joins arrays along a NEW axis.\n",
        "# Creating a batch from individual samples.\n",
        "stacked = np.stack([a, b], axis=0)\n",
        "print(f\"7. Stacked (axis 0 - results in 2x2x2):\\n{stacked.shape}\\n\")\n",
        "\n",
        "# 8. np.repeat(a, repeats, axis): Repeats elements.\n",
        "# Useful for broadcasting a single token mask across the whole sequence.\n",
        "single_val = np.array([[1]])\n",
        "repeated = np.repeat(single_val, 3, axis=1)\n",
        "print(f\"8. Repeated (1x3):\\n{repeated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zNZvl8znt7B",
        "outputId": "6b3cbfb1-27e1-45a6-aa13-1683d990b71b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 2. Shape Manipulation and Transposition ---\n",
            "1. Reshaped (2x12):\n",
            "(2, 12)\n",
            "\n",
            "2. Transposed (0, 2, 1):\n",
            "(2, 4, 3)\n",
            "\n",
            "3. Matrix Transpose:\n",
            "[[1 3]\n",
            " [2 4]]\n",
            "\n",
            "4. Expanded Dims:\n",
            "(1, 4)\n",
            "\n",
            "5. Squeezed:\n",
            "(4,)\n",
            "\n",
            "6. Concatenated (axis 1):\n",
            "[[1. 1. 0. 0.]\n",
            " [1. 1. 0. 0.]]\n",
            "\n",
            "7. Stacked (axis 0 - results in 2x2x2):\n",
            "(2, 2, 2)\n",
            "\n",
            "8. Repeated (1x3):\n",
            "[[1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_1 = np.ones((3, 6 ))\n",
        "print(batch_1)\n",
        "\n",
        "batch_1_expand = np.expand_dims(batch_1, axis=1)\n",
        "print(batch_1_expand)\n",
        "print(batch_1.shape)\n",
        "print(batch_1_expand.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGi-fjwRUP_p",
        "outputId": "f9d4ddfc-7cc4-476b-be02-a7a5d2fe5ffa"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1.]]\n",
            "[[[1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1. 1. 1. 1.]]]\n",
            "(3, 6)\n",
            "(3, 1, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3\n",
        "Mathematical Operations & Linear Algebra"
      ],
      "metadata": {
        "id": "w6gD_OaWoGkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data: Query and Key matrices for a tiny Attention mechanism\n",
        "# Shape: (Seq_Len, Dim) -> (3, 4)\n",
        "Q = np.random.randn(3, 4)\n",
        "K = np.random.randn(3, 4)\n",
        "V = np.random.randn(3, 4)\n",
        "\n",
        "print(\"--- 3. Mathematical Operations & Linear Algebra ---\")\n",
        "\n",
        "# 1. np.matmul(a, b): Matrix product.\n",
        "# Critical for Attention scores (Q * K.T)\n",
        "scores = np.matmul(Q, K.T)\n",
        "print(f\"1. Attention Scores Shape (Q * K^T):\\n{scores.shape}\\n\")\n",
        "print(f\"1. Attention Scores (Q * K^T):\\n{scores}\\n\")\n",
        "\n",
        "# 2. np.dot(a, b): Dot product.\n",
        "# For 1D it's the inner product; for 2D it's similar to matmul.\n",
        "dot_prod = np.dot(Q[0], K[0])\n",
        "print(f\"2. Dot product (first token Q and K):\\n{dot_prod:.4f}\\n\")\n",
        "\n",
        "# 3. np.sum(a, axis, keepdims=True): Summation.\n",
        "# keepdims is vital for broadcasting during normalization.\n",
        "row_sum = np.sum(scores, axis=-1, keepdims=True)\n",
        "print(f\"3. Sum (keepdims=True) shape:\\n{row_sum.shape}\\n\")\n",
        "print(f\"3. Sum (keepdims=True):\\n{row_sum}\\n\")\n",
        "\n",
        "# 4. np.mean / np.std: For Layer Normalization.\n",
        "mean = np.mean(Q, axis=-1, keepdims=True)\n",
        "std = np.std(Q, axis=-1, keepdims=True)\n",
        "print(f\"4. Mean shape for LayerNorm:\\n{mean.shape}\\n\")\n",
        "print(f\"4. Mean for LayerNorm:\\n{mean}\\n\")\n",
        "\n",
        "# 5. np.exp(x): Exponential for Softmax.\n",
        "# softmax = exp(x) / sum(exp(x))\n",
        "exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True)) # subtracted max for stability\n",
        "print(f\"5. Exp (Softmax numerator):\\n{exp_scores[0, :2]}...\\n\")\n",
        "\n",
        "# 6. np.log(x): Natural log for Cross-Entropy loss.\n",
        "probs = np.array([0.1, 0.7, 0.2])\n",
        "loss = -np.log(probs[1]) # Loss for the correct class (index 1)\n",
        "print(f\"6. Log Loss calculation:\\n{loss:.4f}\\n\")\n",
        "\n",
        "# 7. np.sqrt(x): Square root for scaling attention.\n",
        "d_k = Q.shape[-1]\n",
        "scaled_scores = scores / np.sqrt(d_k)\n",
        "print(f\"7. Scaled scores (first row):\\n{scaled_scores[0]}\\n\")\n",
        "\n",
        "# 8. np.linalg.norm(x, axis): Vector magnitude.\n",
        "# Used to calculate Cosine Similarity between embeddings.\n",
        "norm_Q = np.linalg.norm(Q, axis=-1)\n",
        "print(f\"8. Norm of Query vectors:\\n{norm_Q}\\n\")\n",
        "\n",
        "# 9. np.einsum: The \"Swiss Army Knife\" of tensor operations.\n",
        "# Efficiently performs 'Batch Matrix Multiplication': (Batch, Head, Seq, Dim)\n",
        "# Below is a simple transpose + multiply: sum over 'j'\n",
        "einsum_res = np.einsum('ik,jk->ij', Q, K)\n",
        "print(f\"9. Einsum result (matches matmul):\\n{np.allclose(scores, einsum_res)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FRei36WoK54",
        "outputId": "ba0d5632-9000-40a0-d95f-3056219a70e6"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 3. Mathematical Operations & Linear Algebra ---\n",
            "1. Attention Scores Shape (Q * K^T):\n",
            "(3, 3)\n",
            "\n",
            "1. Attention Scores (Q * K^T):\n",
            "[[-1.89926456  0.37706613  2.64701485]\n",
            " [ 0.74450513  0.98959147  0.36480982]\n",
            " [-3.49356701  0.12188847  3.875686  ]]\n",
            "\n",
            "2. Dot product (first token Q and K):\n",
            "-1.8993\n",
            "\n",
            "3. Sum (keepdims=True) shape:\n",
            "(3, 1)\n",
            "\n",
            "3. Sum (keepdims=True):\n",
            "[[1.12481642]\n",
            " [2.09890642]\n",
            " [0.50400746]]\n",
            "\n",
            "4. Mean shape for LayerNorm:\n",
            "(3, 1)\n",
            "\n",
            "4. Mean for LayerNorm:\n",
            "[[-0.19153584]\n",
            " [-0.14276591]\n",
            " [-0.5684074 ]]\n",
            "\n",
            "5. Exp (Softmax numerator):\n",
            "[0.01060659 0.10331748]...\n",
            "\n",
            "6. Log Loss calculation:\n",
            "0.3567\n",
            "\n",
            "7. Scaled scores (first row):\n",
            "[-0.94963228  0.18853307  1.32350742]\n",
            "\n",
            "8. Norm of Query vectors:\n",
            "[1.9236944  1.28270811 2.32549765]\n",
            "\n",
            "9. Einsum result (matches matmul):\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why np.einsum is the \"Final Boss\"\n",
        "In advanced LLM code, you'll often see np.einsum('bhqd,bhkd->bhqk', Q, K). This looks intimidating, but it's just a shorthand for saying: \"Take these 4D tensors and multiply the Query and Key dimensions for every Batch and Head.\" It's incredibly fast and prevents you from having to do multiple reshape and transpose steps."
      ],
      "metadata": {
        "id": "-THbsI29oVW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemini said\n",
        "In NumPy, axis=-1 is a shorthand that means \"the very last dimension.\" It is incredibly common in AI because it allows you to write code that works regardless of how many dimensions your data has (e.g., whether you are processing a single vector, a matrix, or a 4D batch).\n",
        "\n",
        "1. The \"Counting Backwards\" Rule\n",
        "Just like Python list indexing (where list[-1] is the last item), NumPy axis indexing wraps around:\n",
        "\n",
        "axis=0: The first dimension (usually Rows or Batch).\n",
        "\n",
        "axis=1: The second dimension (usually Columns or Sequence).\n",
        "\n",
        "axis=-1: The last dimension, whatever that may be.\n",
        "\n",
        "2. Why use -1 for Summation?\n",
        "In the code snippet you provided, scores usually represents an Attention matrix or a set of Logits.\n",
        "\n",
        "In a matrix of shape (Rows, Columns), axis=-1 targets the Columns.\n",
        "\n",
        "When you sum across the columns, you are collapsing the horizontal values into a single total for each row.\n",
        "\n",
        "Result: You get a \"row sum.\"\n",
        "\n",
        "3. The \"AI Researcher\" Reason\n",
        "In Deep Learning, the last dimension is almost always the \"Feature Dimension\" (like the 512 numbers that represent a word embedding).\n",
        "\n",
        "By using axis=-1, your code becomes rank-agnostic.\n",
        "\n",
        "If you feed it a 2D matrix (Sequence, Features), it sums the features.\n",
        "\n",
        "If you feed it a 3D batch (Batch, Sequence, Features), it still sums the features.\n",
        "\n",
        "If you had hard-coded axis=1, the 3D batch would have summed the sequences instead of the features, likely breaking your math.\n",
        "\n",
        "4. What about keepdims=True?\n",
        "Notice that the code also uses keepdims=True.\n",
        "\n",
        "Normally, np.sum on a (10, 5) matrix returns a (10,) vector (it \"drops\" the summed dimension).\n",
        "\n",
        "With keepdims=True, it returns (10, 1).\n",
        "\n",
        "This is \"vital for broadcasting\" because you can't easily divide a (10, 5) matrix by a (10,) vector. But you can divide a (10, 5) matrix by a (10, 1) matrix—NumPy will automatically stretch that single column to fit all 5 columns."
      ],
      "metadata": {
        "id": "SuIY9fvSrIbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax function\n",
        "\n"
      ],
      "metadata": {
        "id": "ILrpiJxjrLfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Start with scores: (Batch: 2, Seq: 3)\n",
        "scores = np.array([[1.0, 2.0, 3.0],\n",
        "                   [1.0, 1.0, 1.0]])\n",
        "\n",
        "# 2. Sum along the last axis (the row)\n",
        "# Without keepdims, shape becomes (2,)\n",
        "# With keepdims=True, shape stays (2, 1)\n",
        "row_sums = np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
        "print(row_sums)\n",
        "\n",
        "# 3. Divide: (2, 3) / (2, 1)\n",
        "# Broadcasting \"stretches\" the (2, 1) to (2, 3) automatically\n",
        "softmax = np.exp(scores) / row_sums\n",
        "print(softmax)\n",
        "print(softmax.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd3jG9tZr1lx",
        "outputId": "1f35bca0-0b25-4e89-b3f8-73031b8fa5d7"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[30.19287485]\n",
            " [ 8.15484549]]\n",
            "[[0.09003057 0.24472847 0.66524096]\n",
            " [0.33333333 0.33333333 0.33333333]]\n",
            "(2, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The explanation of softmax:\n",
        "axis=-1: Ensures you are summing the tokens in each sequence, even if you add a \"Batch\" or \"Heads\" dimension later.\n",
        "\n",
        "keepdims=True: Keeps the \"1\" in the shape so the division \"lines up\" perfectly across the rows.\n",
        "\n",
        "Here is the mathematical breakdown of that Softmax operation using the logic of axis=-1 and keepdims=True.1. The Raw Scores ($x$)Imagine a 2D matrix where each row is a sequence of token scores:$$x = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 1 & 1 \\end{bmatrix}, \\text{with shape } (2, 3)$$2. ExponentiationFirst, we apply the exponential to every element:$$e^x = \\begin{bmatrix} e^1 & e^2 & e^3 \\\\ e^1 & e^1 & e^1 \\end{bmatrix} \\approx \\begin{bmatrix} 2.72 & 7.39 & 20.08 \\\\ 2.72 & 2.72 & 2.72 \\end{bmatrix}$$3. Summation with axis=-1 and keepdims=TrueWe sum along the last axis (the columns). Because of keepdims=True, the result is a column vector instead of a flat list:$$\\sum e^x = \\begin{bmatrix} 2.72 + 7.39 + 20.08 \\\\ 2.72 + 2.72 + 2.72 \\end{bmatrix} = \\begin{bmatrix} 30.19 \\\\ 8.16 \\end{bmatrix}, \\text{with shape } (2, 1)$$4. The Division (Broadcasting)To get the final probabilities ($P$), we divide the original $(2, 3)$ matrix by the $(2, 1)$ sum vector. NumPy \"broadcasts\" (copies) the column vector 3 times to make the shapes match:$$P = \\frac{\\begin{bmatrix} 2.72 & 7.39 & 20.08 \\\\ 2.72 & 2.72 & 2.72 \\end{bmatrix}}{\\begin{bmatrix} 30.19 & 30.19 & 30.19 \\\\ 8.16 & 8.16 & 8.16 \\end{bmatrix}}$$Final Result$$P = \\begin{bmatrix} 0.09 & 0.24 & 0.67 \\\\ 0.33 & 0.33 & 0.33 \\end{bmatrix}, \\text{with shape } (2, 3)$$Summary of the Logic:The Numerator: $e^{x_i}$The Denominator: $\\sum_{j=1}^n e^{x_j}$The Shape Logic: $(2, 3) / (2, 1) \\rightarrow (2, 3)$"
      ],
      "metadata": {
        "id": "l3pnYlZDsTPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4,\n",
        "Here, we focus on Searching, Sorting, and Logic. These functions are the \"decision-makers.\" You will use them to apply masks (like hiding future tokens in a decoder), find the most likely next word (Argmax), or implement complex sampling strategies like Top-K."
      ],
      "metadata": {
        "id": "THvx0XO_oYSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data: Logits for a vocabulary of 5 words\n",
        "logits = np.array([1.2, -0.5, 3.8, 2.1, 0.5])\n",
        "# Sample attention scores (3x3) for masking\n",
        "scores = np.random.randn(3, 3)\n",
        "\n",
        "print(\"--- 4. Searching, Sorting, and Logic ---\")\n",
        "\n",
        "# 1. np.argmax(a, axis): Find the index of the highest value.\n",
        "# Used to pick the most likely token from the vocabulary.\n",
        "predicted_token_id = np.argmax(logits)\n",
        "print(f\"1. Predicted Token ID: {predicted_token_id}\\n\")\n",
        "\n",
        "# 2. np.argsort(a, axis): Get indices that would sort the array.\n",
        "# Essential for Top-K sampling (finding the 'K' most likely tokens).\n",
        "top_k_indices = np.argsort(logits)[-3:] # Get indices of the top 3\n",
        "print(f\"2. Top-3 Indices (unsorted): {top_k_indices}\\n\")\n",
        "\n",
        "# 3. np.where(condition, x, y): The \"If-Else\" of NumPy.\n",
        "# Used for Causal Masking: if mask is 0, set score to -inf, else keep score.\n",
        "mask = np.tril(np.ones((3, 3))) # Lower triangular matrix\n",
        "masked_scores = np.where(mask == 1, scores, -np.inf)\n",
        "print(f\"3. Masked Attention Scores (Causal):\\n{masked_scores}\\n\")\n",
        "\n",
        "# 4. np.maximum(x, y): Element-wise maximum.\n",
        "# This is exactly how the ReLU (Rectified Linear Unit) activation works.\n",
        "raw_values = np.array([-1, 5, -0.2, 3])\n",
        "relu_output = np.maximum(0, raw_values)\n",
        "print(f\"4. ReLU Output: {relu_output}\\n\")\n",
        "\n",
        "# 5. np.isin(element, test_elements): Membership test.\n",
        "# Useful for identifying if tokens are \"stop words\" or special symbols.\n",
        "vocab_ids = np.array([101, 2045, 300, 102])\n",
        "special_tokens = [101, 102]\n",
        "is_special = np.isin(vocab_ids, special_tokens)\n",
        "print(f\"5. Mask for special tokens: {is_special}\\n\")\n",
        "\n",
        "# 6. np.unique(ar): Find distinct elements.\n",
        "# Used to count how many unique tokens are in a generated sequence.\n",
        "generated_sequence = [5, 5, 2, 8, 2, 1]\n",
        "unique_tokens = np.unique(generated_sequence)\n",
        "print(f\"6. Unique tokens in sequence: {unique_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNQkg4Ezoitk",
        "outputId": "ff6ea0e1-48ac-4858-ad09-d3d925d542ef"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 4. Searching, Sorting, and Logic ---\n",
            "1. Predicted Token ID: 2\n",
            "\n",
            "2. Top-3 Indices (unsorted): [0 3 2]\n",
            "\n",
            "3. Masked Attention Scores (Causal):\n",
            "[[ 0.29362946        -inf        -inf]\n",
            " [-0.16066461 -0.34696028        -inf]\n",
            " [ 0.44715133 -0.11761141 -1.26263281]]\n",
            "\n",
            "4. ReLU Output: [0. 5. 0. 3.]\n",
            "\n",
            "5. Mask for special tokens: [ True False False  True]\n",
            "\n",
            "6. Unique tokens in sequence: [1 2 5 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Importance of np.where .\n",
        "In Transformer models, the np.where function is your primary tool for Masked Self-Attention. By setting illegal future tokens to -np.inf, you ensure that the Softmax function assigns them a probability of exactly $0$, preventing the model from \"cheating\" by looking at the answer before it's supposed to.\n",
        "\n",
        "\n",
        "\n",
        "# Comparison: np.maximum vs. np.max\n",
        "\n",
        " This is a very common point of confusion:np.max(x): Returns the single highest value in the whole array (e.g., $4.0$).np.maximum(0, x): Returns a new array of the same shape, where every negative value is \"clipped\" to zero."
      ],
      "metadata": {
        "id": "v5vQe0dPo2iQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5\n",
        "Here, we dive into Random Sampling. This is what gives LLMs their \"creativity.\" Without these functions, a model would be a purely deterministic machine, always picking the single most likely word.\n",
        "\n",
        "By using random sampling combined with Temperature, we can make the model's output varied and human-like."
      ],
      "metadata": {
        "id": "c7Mlaxi5orPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Seed for reproducibility - vital for debugging!\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"--- 5. Random Sampling ---\")\n",
        "\n",
        "# 1. np.random.randn(d0, d1): Standard Normal distribution (mean=0, var=1).\n",
        "# Commonly used for initializing weight matrices (e.g., Xavier or He init).\n",
        "weights = np.random.randn(4, 4)\n",
        "print(weights)\n",
        "print(f\"1. Random Weight Matrix (Normal):\\n{weights[:2, :2]}...\\n\")\n",
        "\n",
        "# 2. np.random.rand(d0, d1): Uniform distribution [0, 1).\n",
        "# Often used for Dropout layers (generating a mask to drop neurons).\n",
        "dropout_mask = np.random.rand(1, 5) > 0.1  # 10% dropout\n",
        "print(f\"2. Dropout Mask (90% True):\\n{dropout_mask}\\n\")\n",
        "\n",
        "# 3. np.random.randint(low, high, size): Random integers.\n",
        "# Used for sampling random token IDs from a vocabulary or selecting random batches.\n",
        "random_token_ids = np.random.randint(0, 50000, size=(10,))\n",
        "print(f\"3. Randomly Sampled Token IDs:\\n{random_token_ids}\\n\")\n",
        "\n",
        "# 4. np.random.choice(a, size, p): Weighted random sampling.\n",
        "# The core of \"Stochastic Decoding.\" p is the probability distribution from Softmax.\n",
        "vocab = ['The', 'Cat', 'Sat', 'Mat']\n",
        "probs = [0.1, 0.5, 0.2, 0.2]  # Must sum to 1.0\n",
        "next_word = np.random.choice(vocab, size=1, p=probs)\n",
        "print(f\"4. Sampled Word based on probabilities: {next_word}\\n\")\n",
        "\n",
        "# 5. Adding \"Gumbel\" Noise (Advanced):\n",
        "# LLMs often use Gumbel-Max trick for sampling without replacement.\n",
        "logits = np.array([2.0, 1.0, 0.1])\n",
        "noise = np.random.gumbel(0, 1, logits.shape)\n",
        "print(f\"This is the noise: {noise}\")\n",
        "sample_with_noise = np.argmax(logits + noise)\n",
        "print(f\"5. Sampled index with Gumbel noise: {sample_with_noise}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LgzpsiBo_Rl",
        "outputId": "e3c1eba0-6f44-43cb-afce-478b1662d7a1"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 5. Random Sampling ---\n",
            "[[ 0.49671415 -0.1382643   0.64768854  1.52302986]\n",
            " [-0.23415337 -0.23413696  1.57921282  0.76743473]\n",
            " [-0.46947439  0.54256004 -0.46341769 -0.46572975]\n",
            " [ 0.24196227 -1.91328024 -1.72491783 -0.56228753]]\n",
            "1. Random Weight Matrix (Normal):\n",
            "[[ 0.49671415 -0.1382643 ]\n",
            " [-0.23415337 -0.23413696]]...\n",
            "\n",
            "2. Dropout Mask (90% True):\n",
            "[[ True  True  True  True  True]]\n",
            "\n",
            "3. Randomly Sampled Token IDs:\n",
            "[19118 35773  1899  1267 31551 11394  3556  3890 41606 30740]\n",
            "\n",
            "4. Sampled Word based on probabilities: ['Cat']\n",
            "\n",
            "This is the noise: [ 4.31595979 -1.04759856  0.18816036]\n",
            "5. Sampled index with Gumbel noise: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In NumPy, using a comparison operator like the greater-than sign (>) on an array triggers Boolean Broadcasting.\n",
        "Instead of returning a single True or False, it performs the check on every individual element and returns a new array of the same shape filled with Booleans.\n",
        "1. How it works (Step-by-Step)Generation: np.random.rand(1, 5) creates 5 random numbers between $0$ and $1$ (e.g., [0.05, 0.82, 0.12, 0.01, 0.95]).Comparison: The > 0.1 is applied to each number.Result:$0.05 > 0.1 \\rightarrow$ False$0.82 > 0.1 \\rightarrow$ True$0.12 > 0.1 \\rightarrow$ True$0.01 > 0.1 \\rightarrow$ False$0.95 > 0.1 \\rightarrow$ True\n",
        "2. Why this represents \"Dropout\"In a Neural Network, Dropout is a technique where you randomly \"kill\" neurons during training to prevent the model from becoming too reliant on specific connections.The Threshold (0.1): Since rand() is uniform, there is exactly a $10\\%$ chance a number will be less than $0.1$.The Mask: By using >, we create a filter where $90\\%$ of the values are True (Keep) and $10\\%$ are False (Drop).The Application: In a real model, you would multiply your weights by this mask. Because True acts like $1$ and False acts like $0$, the $10\\%$ of neurons associated with False values effectively disappear for that training step.3. Quick Pro-Tip: MemoryBoolean masks are incredibly memory-efficient. If you need to save space, you can convert these to integers using .astype(int), turning True/False into 1/0."
      ],
      "metadata": {
        "id": "p-ZkvG53xOKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision (CV)\n",
        "NumPy's role shifts slightly in Computer Vision. While LLMs are about sequences and logic, CV is about spatial geometry and channel manipulation. You'll be dealing with 4D tensors: $(Batch, Height, Width, Channels)$.Here is the \"CV Research Extension\" for your NumPy library.7. Computer Vision Specific OperationsThese functions are essential for image augmentation, color-space shifts, and coordinate transformations."
      ],
      "metadata": {
        "id": "WGR-TTyTpm3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample Image: 1 batch, 64x64 pixels, 3 color channels (RGB)\n",
        "# Shape: (1, 64, 64, 3)\n",
        "image = np.random.randint(0, 255, (1, 64, 64, 3), dtype=np.uint8)\n",
        "\n",
        "print(\"--- 7. Computer Vision Research Operations ---\")\n",
        "\n",
        "# 1. np.flip(a, axis): Reverse array elements.\n",
        "# Used for \"Horizontal Flip\" data augmentation.\n",
        "flipped_img = np.flip(image, axis=2) # axis 2 is Width\n",
        "print(f\"1. Flipped image shape: {flipped_img.shape}\\n\")\n",
        "\n",
        "# 2. np.pad(array, pad_width, mode): Padding.\n",
        "# Critical for constant-size inputs or preparing for convolutions.\n",
        "padded_img = np.pad(image, ((0,0), (2,2), (2,2), (0,0)), mode='constant')\n",
        "print(f\"2. Padded image (64x64 -> 68x68): {padded_img.shape}\\n\")\n",
        "\n",
        "# 3. np.roll(a, shift, axis): Cyclic shift.\n",
        "# Used in \"Shift-Invariant\" testing or certain Augmentation techniques.\n",
        "shifted_img = np.roll(image, shift=10, axis=1)\n",
        "print(f\"3. image shifted 10 pixels down. {shifted_img}\\n\")\n",
        "\n",
        "# 4. np.clip(a, a_min, a_max): Limit values.\n",
        "# Essential after brightness adjustments to keep pixels in [0, 255] range.\n",
        "bright_img = image.astype(float) + 50\n",
        "cleaned_img = np.clip(bright_img, 0, 255).astype(np.uint8)\n",
        "print(f\"4. Max value after clipping: {np.max(cleaned_img)}\\n\")\n",
        "\n",
        "# 5. np.tile(a, reps): Construct an array by repeating.\n",
        "# Used to repeat a single-channel mask across 3 RGB channels.\n",
        "mask = np.random.rand(64, 64, 1) > 0.5\n",
        "triple_mask = np.tile(mask, (1, 1, 3))\n",
        "print(f\"5. Tiled mask shape: {triple_mask.shape}\\n\")\n",
        "\n",
        "# 6. np.moveaxis(a, source, destination): Move axes to specific positions.\n",
        "# Crucial for switching between (H, W, C) and (C, H, W) for PyTorch compatibility.\n",
        "pytorch_format = np.moveaxis(image, -1, 1) # Moves channels to index 1\n",
        "print(f\"6. PyTorch format (B, C, H, W): {pytorch_format.shape}\\n\")\n",
        "\n",
        "# 7. np.meshgrid(x, y): Coordinate grids.\n",
        "# Used for creating spatial positional encodings or sampling grids.\n",
        "x = np.linspace(0, 1, 64)\n",
        "y = np.linspace(0, 1, 64)\n",
        "xv, yv = np.meshgrid(x, y)\n",
        "print(f\"7. Meshgrid shape (for coordinate-based models): {xv.shape}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s1sMN4Apxhe",
        "outputId": "c0fb4de2-d0fc-4023-9a91-8ef88e1bf49f"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 7. Computer Vision Research Operations ---\n",
            "1. Flipped image shape: (1, 64, 64, 3)\n",
            "\n",
            "2. Padded image (64x64 -> 68x68): (1, 68, 68, 3)\n",
            "\n",
            "3. image shifted 10 pixels down. [[[[198 160 111]\n",
            "   [156  88 223]\n",
            "   [ 42 222 250]\n",
            "   ...\n",
            "   [ 48 144 122]\n",
            "   [ 88 230 149]\n",
            "   [ 34 210 110]]\n",
            "\n",
            "  [[ 26  27 159]\n",
            "   [ 64 245 128]\n",
            "   [ 60 218 194]\n",
            "   ...\n",
            "   [190 212 167]\n",
            "   [245  50  44]\n",
            "   [215 219 136]]\n",
            "\n",
            "  [[237 235 202]\n",
            "   [140   2  81]\n",
            "   [179 167 214]\n",
            "   ...\n",
            "   [234 193 186]\n",
            "   [ 95  53 246]\n",
            "   [200 176  18]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[222 224 179]\n",
            "   [164   2   0]\n",
            "   [ 74 195 194]\n",
            "   ...\n",
            "   [167  72 140]\n",
            "   [ 45 103   3]\n",
            "   [ 78 218  97]]\n",
            "\n",
            "  [[ 75 244  46]\n",
            "   [166  72 127]\n",
            "   [ 35  72  76]\n",
            "   ...\n",
            "   [193 185 178]\n",
            "   [192 208 170]\n",
            "   [ 49  62 223]]\n",
            "\n",
            "  [[ 52  73 121]\n",
            "   [ 42 160 138]\n",
            "   [232   7 169]\n",
            "   ...\n",
            "   [245 244 238]\n",
            "   [139 171  67]\n",
            "   [226  74  74]]]]\n",
            "\n",
            "4. Max value after clipping: 255\n",
            "\n",
            "5. Tiled mask shape: (64, 64, 3)\n",
            "\n",
            "6. PyTorch format (B, C, H, W): (1, 3, 64, 64)\n",
            "\n",
            "7. Meshgrid shape (for coordinate-based models): (64, 64)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand np.flip, think of it as a mirror reflection. When you flip an image, you aren't moving the pixels to random places; you are just reversing the order in which they appear along a specific direction.The \"Mirror\" ExampleImagine a tiny $2 \\times 3$ grayscale image (2 pixels high, 3 pixels wide). Each number is a pixel brightness.Pythonimport numpy as np\n",
        "\n",
        "# A 2D \"image\" (Height=2, Width=3)\n",
        "img = np.array([[10, 20, 30],\n",
        "                [40, 50, 60]])\n",
        "1. Horizontal Flip (axis=1)If we flip along the Width (the columns), it’s like looking in a mirror. The first pixel in a row becomes the last.Pythonhoriz_flip = np.flip(img, axis=1)\n",
        "# Result:\n",
        "# [[30, 20, 10],\n",
        "#  [60, 50, 40]]\n",
        "2. Vertical Flip (axis=0)If we flip along the Height (the rows), it’s like a reflection in a lake. The top row moves to the bottom.Pythonvert_flip = np.flip(img, axis=0)\n",
        "# Result:\n",
        "# [[40, 50, 60],\n",
        "#  [10, 20, 30]]\n",
        "Why the code says axis=2?In Computer Vision research, images are usually 4D tensors:$(Batch, Height, Width, Channels)$Axis 0: Batch (which image in the group)Axis 1: Height (up and down)Axis 2: Width (left and right)Axis 3: Channels (Red, Green, Blue)When the code uses np.flip(image, axis=2), it is telling NumPy: \"Go to the Width dimension and reverse the order of the pixels.\" This creates a Horizontal Flip, which is the most common data augmentation technique because a cat is still a cat whether it's looking left or right!A Quick WarningBe careful not to flip axis=3 (Channels). If you do, your Red pixels might become Blue pixels, and your \"Horizontal Flip\" will accidentally turn into a \"Color Space Swap,\" which might confuse your model!"
      ],
      "metadata": {
        "id": "LMjiEJuiArU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding in Computer Vision,\n",
        "Padding is like adding a \"border\" around an image. This is usually done so that when a Convolutional Neural Network (CNN) slides a filter over the image, the edges don't get \"shrunk\" or lost.Here is the breakdown of the three key arguments in your code:1. array (The Image)This is your input data. In your example, it is a 4D tensor with the shape (Batch, Height, Width, Channels).\n",
        "\n",
        "2. pad_width (The \"How Much\" Argument)This is often the most confusing part. It is a tuple of tuples, where each inner tuple corresponds to one dimension (axis) of your array.Format: ((before_axis0, after_axis0), (before_axis1, after_axis1), ...)In your code: ((0,0), (2,2), (2,2), (0,0))(0,0) (Batch): Add 0 pixels to the start and end. (Don't add more images).(2,2) (Height): Add 2 rows of pixels to the top and 2 to the bottom.(2,2) (Width): Add 2 columns of pixels to the left and 2 to the right.(0,0) (Channels): Add 0 to the color channels. (Don't add extra colors).Result: Since you added 2 to each side of the Height and Width, a $64 \\times 64$ image becomes $68 \\times 68$ ($64 + 2 + 2 = 68$).\n",
        "\n",
        "3. mode='constant' (The \"What Color\" Argument)This defines what values the new \"border\" pixels should have.constant: Fills the border with a constant value (default is 0, or \"black\").reflect: Mirrors the pixels from inside the image (great for keeping textures natural).edge: Stretches the very last pixel of the image outward.Why this is \"Critical\" for ResearchMaintaining Resolution: Without padding, a $3 \\times 3$ convolution filter would turn your $64 \\times 64$ image into a $62 \\times 62$ image. After many layers, your image would disappear!Edge Detection: Padding ensures the model \"sees\" the pixels at the very edge of the frame as many times as it sees the pixels in the center.Pro-Tip: Adding a Specific ColorIf you want to pad with white pixels (value 255) instead of black, you add a constant_values argument:Pythonpadded = np.pad(image, ((0,0), (2,2), (2,2), (0,0)),\n",
        "                mode='constant', constant_values=255)"
      ],
      "metadata": {
        "id": "oyxA8CU7Ci16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# np.roll is a circular shift operation.\n",
        "\n",
        "Unlike padding (which adds new pixels) or cropping (which removes them), rolling takes pixels that fall off one edge and \"wraps\" them back around to the opposite edge.\n",
        "\n",
        "Think of it like a rotating conveyor belt or a globe: if you keep walking East, you eventually end up back where you started.\n",
        "\n",
        "The Basic Example\n",
        "Imagine a 1D sequence of numbers: [10, 20, 30, 40, 50]\n",
        "\n",
        "1. Shift by 1 (shift=1)\n",
        "Everything moves right. The last element (50) wraps around to become the first.\n",
        "\n",
        "Result: [50, 10, 20, 30, 40]\n",
        "\n",
        "2. Shift by -1 (shift=-1)\n",
        "Everything moves left. The first element (10) wraps around to become the last.\n",
        "\n",
        "Result: [20, 30, 40, 50, 10]\n",
        "\n",
        "Breaking Down your Code\n",
        "shifted_img = np.roll(image, shift=10, axis=1)\n",
        "\n",
        "image: Your input tensor (usually (Height, Width, Channels)).\n",
        "\n",
        "shift=10: Move the content 10 units forward.\n",
        "\n",
        "axis=1: Apply this to the vertical axis (Height).\n",
        "\n",
        "What happens to the image?\n",
        "The top 10 rows of the image are pushed off the bottom and reappear at the top. If your image was a picture of a person, their head might disappear from the top of the frame and pop up at the bottom of their feet.\n",
        "\n",
        "Why use this in AI Research?\n",
        "Shift-Invariance Testing: You want to ensure your model recognizes a \"Cat\" whether it's in the center or shifted slightly to the edge.\n",
        "\n",
        "Symmetry in Data: In physics-based ML (like weather patterns), data often \"wraps\" around the globe, making np.roll a mathematically accurate way to augment that data.\n",
        "\n",
        "Cyclic Features: Useful for time-series data that repeats, like 24-hour temperature cycles."
      ],
      "metadata": {
        "id": "pE5cLfWgKauL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. The \"Wrap-around\" (Overflow) Problem\n",
        "\n",
        "Most images are stored in 8-bit unsigned integers (uint8). This format only has 256 possible values ($0$ to $255$).If you perform math on uint8 and the result exceeds $255$, it doesn't stop at $255$—it wraps around back to $0$.Example: 250 + 10 in uint8 doesn't equal 260. It equals 4 ($260 - 256$).Visual Result: A bright white sky in your photo would suddenly turn pitch black because the values \"tripped\" over the maximum limit.By converting to float, you move into a number system that can handle values like $260$, $300$, or $10,000$ without breaking. This lets you do your math safely before \"clipping\" it back down to the $0-255$ range.\n",
        "# 2. The Arguments in np.clipnp.clip:\n",
        "It forces every number in an array to stay within a specific boundary.Syntax: np.clip(a, a_min, a_max)a (The Array): The input data you want to restrict (e.g., your brightened float image).a_min (The Floor): The minimum allowed value. Anything smaller than this becomes this value.In images: We usually set this to 0.a_max (The Ceiling): The maximum allowed value. Anything larger than this becomes this value.In images: We usually set this to 255.Basic Logic ExampleImagine an array of \"broken\" image values after adding brightness:pixels = [10, 280, -5, 125]Python# Force values to stay between 0 and 255\n",
        "cleaned = np.clip(pixels, 0, 255)\n",
        "\n",
        "What happens inside:10 stays 10 (It's within bounds).280 becomes 255 (It hit the ceiling).-5 becomes 0 (It hit the floor).125 stays 125.Why np.clip is better than just converting back?If you simply converted 280.0 back to uint8 without clipping, you would get that \"wrap-around\" error (the pixel would turn into 24). By clipping first, you ensure that \"super bright\" pixels stay \"pure white\" ($255$) instead of turning into random dark colors."
      ],
      "metadata": {
        "id": "z6Pgv_OuMM0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# np.tile:\n",
        "np.tile and np.repeat handle creating a batch of images.Suppose you have one grayscale image $A$ of shape $(2, 2)$ and you want to create a batch of 3 identical images.1. The np.repeat approachnp.repeat works at the element level. If you repeat along the first axis (rows), it duplicates each row before moving to the next.Pythonimport numpy as np\n",
        "a = np.array([[1, 2],\n",
        "              [3, 4]])\n",
        "\n",
        "# Repeat rows 3 times\n",
        "repeated = np.repeat(a, 3, axis=0)\n",
        "Result Shape: $(6, 2)$Result: Each row is duplicated 3 times in a row.Python[[1, 2],\n",
        " [1, 2],\n",
        " [1, 2],\n",
        " [3, 4],\n",
        " [3, 4],\n",
        " [3, 4]]\n",
        "2. The np.tile approachnp.tile works at the structure level. It treats the entire array as a single \"tile\" and lays it down repeatedly.Python# Tile the whole block 3 times vertically\n",
        "tiled = np.tile(a, (3, 1))\n",
        "Result Shape: $(6, 2)$Result: The entire image pattern repeats 3 times.Python[[1, 2],\n",
        " [3, 4],\n",
        " [1, 2],\n",
        " [3, 4],\n",
        " [1, 2],\n",
        " [3, 4]]\n",
        "Summary Table: Creating Identical BatchesGoalBest FunctionReasonDuplicate a mask for RGBnp.tileYou want the whole \"mask pattern\" copied for each color channel.Magnify an image (Zoom)np.repeatYou want to duplicate each individual pixel to make it look \"bigger\" (Nearest-Neighbor scaling).Create a batch of 32 identical samplesnp.tileYou want 32 complete copies of the image data, one after another.🚀\n",
        "\n",
        "Final NumPy Performance Tip\n",
        "\n",
        "If you are doing this to prepare data for a model, consider using np.broadcast_to instead:Python# Does not use extra RAM; just \"pretends\" the array is bigger\n",
        "batched = np.broadcast_to(a, (3, 2, 2))\n",
        "\n",
        "This is significantly faster and uses less memory because it doesn't actually copy the data—it just changes the metadata (strides) of the array."
      ],
      "metadata": {
        "id": "6_XJZnPeOHPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important Note:\n",
        "np.moveaxis is the CV \"Secret Weapon\"\n",
        "In Computer Vision, libraries never agree on shape.\n",
        "\n",
        "OpenCV & TensorFlow: Expect (Height, Width, Channels).\n",
        "\n",
        "PyTorch: Expect (Channels, Height, Width).\n",
        "\n",
        "If you try to feed an image into a PyTorch model without using np.moveaxis (or np.transpose), your model will try to treat the width of your image as the color channels, and everything will crash."
      ],
      "metadata": {
        "id": "JE_Ygi1aqHmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Area: Data Analysis & Stats\n",
        "Functions for evaluating your models (both LLM and CV)."
      ],
      "metadata": {
        "id": "9vpjH3CLqXgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. np.percentile(a, q): Compute the q-th percentile.\n",
        "# Used to find the \"95th percentile\" of token lengths to set a max_length.\n",
        "lengths = np.array([10, 15, 200, 12, 18, 500])\n",
        "max_len = np.percentile(lengths, 95)\n",
        "print(f\"8. 95th Percentile length: {max_len}\\n\")\n",
        "\n",
        "# 9. np.nan_to_num(x): Replace NaNs with zero.\n",
        "# Prevents a single 'broken' weight from crashing your whole training run.\n",
        "corrupted_data = np.array([1.0, np.nan, np.inf])\n",
        "clean_data = np.nan_to_num(corrupted_data)\n",
        "print(f\"9. Cleaned NaN/Inf: {clean_data}\\n\")\n",
        "\n",
        "# 10. np.savez_compressed('file.npz', a=arr1, b=arr2):\n",
        "# The professional way to save massive datasets efficiently.\n",
        "np.savez_compressed('model_data.npz', weights=weights, images=image)\n",
        "print(f\"10. Data saved to compressed .npz format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WTnP5OcqdVU",
        "outputId": "40c64e41-caba-445d-8494-9d79a44658a2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8. 95th Percentile length: 425.0\n",
            "\n",
            "9. Cleaned NaN/Inf: [1.00000000e+000 0.00000000e+000 1.79769313e+308]\n",
            "\n",
            "10. Data saved to compressed .npz format.\n"
          ]
        }
      ]
    }
  ]
}