üß† NumPy for Deep LearningMastering the mathematical engine behind modern AI. > This repository is a comprehensive guide to using NumPy specifically for Deep Learning applications‚Äîfrom tensor manipulation to implementing core functions like Softmax, ReLU, and Gumbel-Max.üöÄ OverviewDeep Learning is essentially Linear Algebra + Calculus + Code. While high-level frameworks like PyTorch and TensorFlow are standard, understanding the underlying NumPy implementation is crucial for debugging, custom layer creation, and research.Key Concepts Covered:Tensor Surgery: Advanced slicing, np.stack vs np.concatenate, and axis manipulation.Activation Functions: Vectorized implementation of ReLU, Leaky ReLU, and Softmax.Data Augmentation: Using np.flip, np.roll, and np.pad for image preprocessing.Stochastic Tricks: Implementing the Gumbel-Max Trick for differentiable sampling.Normalization: Calculating mean/variance across specific axes using keepdims.üõ†Ô∏è Installation & SetupTo run the notebooks locally, clone the repo and install the dependencies:Bashgit clone https://github.com/SYEDFAIZAN1987/NumpyForDeepLearning.git
cd NumpyForDeepLearning
pip install numpy jupyterlab
üìñ Deep Dive: Core Implementations1. The Softmax FunctionEssential for multi-class classification, implemented using the axis=-1 and keepdims pattern to support batch processing.$$\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}$$2. ReLU Activation (Non-Linearity)Implemented via np.maximum(0, x), effectively "deactivating" neurons with negative signals.3. Gumbel-Max SamplingA technique used in LLMs and Reinforcement Learning to sample from categorical distributions.Python# Adding Gumbel noise to logits
noise = np.random.gumbel(0, 1, logits.shape)
sample = np.argmax(logits + noise)
üìÇ Repository StructurePlaintext‚îú‚îÄ‚îÄ NumpyNotes.ipynb      # Main workbook with code & explanations
‚îú‚îÄ‚îÄ README.md             # Documentation
‚îî‚îÄ‚îÄ .gitignore            # Clean git history (prevents .ipynb_checkpoints)
ü§ù ContributingFound a more efficient way to implement a layer? Open a PR! I'm always looking to optimize these mathematical implementations.Fork the ProjectCreate your Feature Branch (git checkout -b feature/AmazingFeature)Commit your Changes (git commit -m 'Add some AmazingFeature')Push to the Branch (git push origin feature/AmazingFeature)Open a Pull Requestüë§ AuthorSyed Faizan * GitHub: @SYEDFAIZAN1987LinkedIn: [Your LinkedIn Profile Link]
